{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras循环神经网络.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjvIH21FGTjE5ab8/z9wwJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euh3ztsYcpt8",
        "colab_type": "text"
      },
      "source": [
        "# **Keras的循环神经网络（RNN）**\n",
        "\n",
        "### **介绍**\n",
        "\n",
        "循环神经网络（RNN）是一类神经网络，对于建模序列数据（例如时间序列或自然语言）非常有力。\n",
        "\n",
        "在示意图上，RNN层使用`for`循环在序列的时间步上进行迭代，同时维护内部状态，该状态对已执行的时间步的信息进行编码。\n",
        "\n",
        "Keras RNN API的设计重点是：\n",
        "\n",
        "**易于使用**：内置的[`keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)，[`keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)，[`keras.layers.GRU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)层使你能够快速构建循环模型，而不必进行困难的配置选择。\n",
        "\n",
        "**易于定制**：你还可以使用自定义行为定义自己的RNN单元层（`for`循环的内部），并将其与通用`keras.layers.RNN`层（`for`循环本身）一起使用。这使你能够以最少的代码灵活且快速地原型化不同的研究思路。\n",
        "\n",
        "### 引入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3A4UqrEciP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miOxVoKQdkLW",
        "colab_type": "text"
      },
      "source": [
        "### **内置RNN层：一个简单的示例**\n",
        "\n",
        "Keras中有三个内置的RNN层：\n",
        "\n",
        "1. [`keras.layers.SimpleRNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)，一个完全连接的RNN，它将上一个时间步的输出将馈送到下一个时间步。\n",
        "\n",
        "2. [`keras.layers.GRU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)，在[Cho et al.,2014](https://arxiv.org/abs/1406.1078)中首先被提出 。\n",
        "\n",
        "3. [`keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)，在[Hochreiter & Schmidhuber, 1997.](https://www.bioinf.jku.at/publications/older/2604.pdf)中首先被提出 。\n",
        "\n",
        "2015年初，Keras拥有LSTM和GRU的第一个可重用的开源Python实现。\n",
        "\n",
        "下面是一个简单的`Sequential`模型示例，该模型处理整数序列，将每个整数嵌入到64维向量中，然后使用`LSTM`层处理向量序列。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzfVGKuBdo8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "86bcaf80-bfb8-4fcf-dcd6-e8eaeaf88ffb"
      },
      "source": [
        "model = keras.Sequential()\n",
        "# 添加一个输入单词量大小为1000，输出嵌入尺寸为64的嵌入层\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# 添加一个128个内部单元的LSTM层\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# 添加一个10个内部单元的密度层\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsrz_2hdquq",
        "colab_type": "text"
      },
      "source": [
        "内置RNN支持许多有用的功能：\n",
        "\n",
        "+ 通过`dropout`和`recurrent_dropout`参数进行循环dropout\n",
        "+ 可以通过`go_backwards`参数反向处理输入序列\n",
        "+ 通过`unroll`参数进行循环展开（在CPU上处理短序列时可能更快）\n",
        "+ ...和更多。\n",
        "\n",
        "有关更多信息，请参见[RNN API文档](https://keras.io/api/layers/recurrent_layers/)。\n",
        "\n",
        "### **输出和状态**\n",
        "\n",
        "默认情况下，RNN层的输出包含每个样本对应的一个向量。该向量是最后一个时间步相对应的RNN单元输出，其中包含有关整个输入序列的信息。此输出的形状为`(batch_size, units)`，其中`units`对应于传递给层构造函数的`units`参数。\n",
        "\n",
        "如果你设置`return_sequences=True`，则RNN层还可以返回每个样本的整个输出序列（每个样本的每个时间步的一个向量），此输出的形状是`(batch_size, timesteps, units)` 。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F61xsqIedvyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "909cd8d4-621c-414a-840f-249e02ac49ff"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# GRU的输出将是形状为（batch_size，timesteps，256）的3D张量\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# SimpleRNN的输出将是形状为（batch_size，128）的2D张量\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 256)         247296    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 361,866\n",
            "Trainable params: 361,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P6AfGPFdxoS",
        "colab_type": "text"
      },
      "source": [
        "此外，RNN层可以返回其最终的内部状态，返回的状态可用于稍后恢复RNN执行或[初始化另一个RNN](https://arxiv.org/abs/1409.3215)，此设置通常用于编码器-解码器的Seq2Seq模型，其中编码器的最终状态用作解码器的初始状态。\n",
        "\n",
        "要将RNN层配置为返回其内部状态，需要在创建层时将`return_state`参数设置为`True`。值得注意的是，`LSTM`具有2个状态张量，但`GRU`仅具有1个。\n",
        "\n",
        "要配置层的初始状态，只需使用关键字参数`initial_state`来调用层即可。请注意，状态的形状需要与层的单元大小匹配，如以下示例所示。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIghsv75dzQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "448fe30c-79e6-4abf-8bfe-77f93ef7d6f4"
      },
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# 返回除输出外的状态\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# 将这两种状态作为初始状态传递到新的LSTM层\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()\n",
        " "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 64)     64000       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 64)     128000      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "encoder (LSTM)                  [(None, 64), (None,  33024       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  (None, 64)           33024       embedding_3[0][0]                \n",
            "                                                                 encoder[0][1]                    \n",
            "                                                                 encoder[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           650         decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 258,698\n",
            "Trainable params: 258,698\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC96aaRMd0w5",
        "colab_type": "text"
      },
      "source": [
        "### **RNN层和RNN单元**\n",
        "\n",
        "除了内置的RNN层之外，RNN API还提供了单元级API。与处理整批量输入序列的RNN层不同，RNN单元仅处理单个时间步。\n",
        "\n",
        "该单元在RNN层的`for`循环的内部。在[`keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)层中包装一个单元，可以为你提供能够处理批量序列的层，例如RNN(LSTMCell(10)) 。\n",
        "\n",
        "从数学角度来看，`RNN(LSTMCell(10))`产生与`LSTM(10)`相同的结果。实际上，在TF v1.x中该层的实现只是创建相应的RNN单元并将其包装在RNN层中。但是，使用内置的`GRU`和`LSTM`层可以使用CuDNN，您可能会看到更好的性能。\n",
        "\n",
        "内置三个RNN单元，每个单元对应于匹配的RNN层。\n",
        "\n",
        "+ [`keras.layers.SimpleRNNCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNNCell)对应于SimpleRNN层。\n",
        "\n",
        "+ [`keras.layers.GRUCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell)对应于GRU层。\n",
        "\n",
        "+ [`keras.layers.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)对应于LSTM层。\n",
        "\n",
        "单元抽象以及通用的`keras.layers.RNN`类使得实现自定义RNN体系结构变得非常容易。\n",
        "\n",
        "### **跨批状态**\n",
        "\n",
        "当处理非常长的序列（可能是无限的）时，你可能需要使用跨批状态的模式。\n",
        "\n",
        "通常，每次看到新批次时，都会重置RNN层的内部状态（即，假定该层看到的每个样本都独立于过去的），该层仅在处理给定样本时保持状态。\n",
        "\n",
        "如果你有很长的序列，则将它们分成较短的序列，然后将这些较短的序列依次传入RNN层而不重置该层的状态将非常有用。这样，即使一次只看到一个子序列，该层也可以保留有关整个序列的信息。\n",
        "\n",
        "你可以通过在构造函数中设置`stateful=True`来实现。\n",
        "\n",
        "如果有一个序列`s = [t0, t1, ... t1546, t1547]` ，则将其拆分为"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjx4SDsSd-cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = [t0, t1, ... t100]\n",
        "s2 = [t101, ... t201]\n",
        "...\n",
        "s16 = [t1501, ... t1547]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJx4ujrRd_ST",
        "colab_type": "text"
      },
      "source": [
        "然后，你可以通过以下方式进行处理："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyHzrtbSeART",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "for s in sub_sequences:\n",
        "  output = lstm_layer(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv0-Al7veCDT",
        "colab_type": "text"
      },
      "source": [
        "当你想要清除状态时，可以使用`layer.reset_states()`。\n",
        "\n",
        "注：在此设置中，给定批次中的样本i将被假定为前面批次样本的延续。这意味着所有批次应包含相同数量的样本（批次大小）。例如，如果一个批次包含`[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`，则下一个批次应包含`[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]` 。\n",
        "\n",
        "下面是一个完整的示例："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAc30K8yeHp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "output = lstm_layer(paragraph3)\n",
        "\n",
        "# reset_states())会将缓存的状态重置为原始的initial_state。\n",
        "# 如果未提供initial_state，则默认情况下将使用零状态。\n",
        "lstm_layer.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqetbjUdeIzg",
        "colab_type": "text"
      },
      "source": [
        "### **RNN状态重用**\n",
        "\n",
        "RNN层的状态记录不包含在`layer.weights()`中。如果你想重用RNN层的状态，则可以通过**layer.states**检索状态值并将其用作新层的初始状态，就像函数式API（或子类化模型）中的`new_layer(inputs, initial_state=layer.states)`。\n",
        "\n",
        "请注意，在这种情况下可能不会使用`Sequential`模型，因为它仅支持具有单个输入和输出的层，初始状态的额外输入使得无法使用`Sequential`模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pin7y_D8eNUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "\n",
        "existing_state = lstm_layer.states\n",
        "\n",
        "new_lstm_layer = layers.LSTM(64)\n",
        "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjWms7uxePFJ",
        "colab_type": "text"
      },
      "source": [
        "### **双向RNN**\n",
        "\n",
        "RNN模型不仅可以从头到尾处理序列，而且可以反向处理序列。通常情况下，对于时间序列以外的序列（例如文本），双向RNN效果会更好。例如，要预测句子中的下一个单词，通常使单词具有上下文。\n",
        "\n",
        "Keras提供了一个简单的API，用于构建此类双向RNN：[`keras.layers.Bidirectional`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)包装器。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxR4H8u6eSJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1895aa3a-c5fd-4c76-d5b4-a9e5b9b4909b"
      },
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
        ")\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional (Bidirectional (None, 5, 128)            38400     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQMrlp5eT-g",
        "colab_type": "text"
      },
      "source": [
        "`Bidirectional`将复制传入的RNN层，并翻转新层的`go_backwards`字段，以便它能够处理输入的反序列。\n",
        "\n",
        "默认情况下，`Bidirectional` RNN的输出将是前向层输出和后向层输出的总和。如果你需要其他合并操作（如concatenation），请在`Bidirectional`的构造函数中更改`merge_mode`属性。有关`Bidirectional`更多详细信息，请检查[API文档](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional/)。\n",
        "\n",
        "### **性能优化和CuDNN内核**\n",
        "\n",
        "在TensorFlow 2.0中，内置的LSTM和GRU层已更新为在GPU可用时，默认使用CuDNN内核。通过此更改，先前的`keras.layers.CuDNNLSTM/CuDNNGRU`层已被弃用，你可以构建模型而不必关心模型所运行的硬件。\n",
        "\n",
        "由于CuDNN内核是根据某些假设构建的，因此，这意味着**如果你更改内置LSTM或GRU层的默认设置，则该层将无法使用CuDNN内核**。例如：\n",
        "\n",
        "+ 将`activation`方法从`tanh`更改为其他激活方法。\n",
        "+ 将`recurrent_activation`方法从`sigmoid`更改为其他方法。\n",
        "+ 设置`recurrent_dropout` > 0。\n",
        "+ 将`unroll`设置为True，这将强制LSTM/GRU把内部的`tf.while_loop`分解为`for`循环。\n",
        "+ 将`use_bias`设置为False。\n",
        "+ 当输入数据未严格进行右填充时，使用掩码（如果掩码对应于严格的右填充数据，则仍可以使用CuDNN，这是最常见的情况）。\n",
        "\n",
        "有关约束的详细列表，请参阅[LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM/)和[GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU/)层的文档。\n",
        "\n",
        "#### **在CuDNN内核可用时使用它**\n",
        "让我们编写一个简单的LSTM模型来演示性能差异。\n",
        "\n",
        "我们将使用MNIST数字的行序列作为输入序列（将每个像素行作为时间步进行处理），并预测该数字的标签。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLCVCVdleZ5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "# 每个MNIST图像批量都是一个形状为（batch_size，28，28）的张量\n",
        "# 每个输入序列的大小为（28，28）（将高度视为时间）\n",
        "input_dim = 28\n",
        "\n",
        "units = 64\n",
        "output_size = 10  # labels are from 0 to 9\n",
        "\n",
        "# 创建RNN模型\n",
        "def build_model(allow_cudnn_kernel=True):\n",
        "    # CuDNN仅在层级别时可用，而在单元级别时不可用。\n",
        "    # 这意味着`LSTM(units)`将使用CuDNN内\n",
        "    # 核，而RNN(LSTMCell(units))将在非CuDNN内核上运行。\n",
        "    if allow_cudnn_kernel:\n",
        "        # 具有默认选项的LSTM层使用CuDNN\n",
        "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
        "    else:\n",
        "        # 在RNN层中包装LSTMCell不会使用CuDNN\n",
        "        lstm_layer = keras.layers.RNN(\n",
        "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
        "        )\n",
        "    model = keras.models.Sequential(\n",
        "        [\n",
        "            lstm_layer,\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.Dense(output_size),\n",
        "        ]\n",
        "    )\n",
        "    return model\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKPlxXwhea_7",
        "colab_type": "text"
      },
      "source": [
        "让我们加载MNIST数据集："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B30f7EkUecwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "432fdb7e-1073-46e9-a5d6-c22a092791eb"
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "sample, sample_label = x_train[0], y_train[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXaQa1ped0c",
        "colab_type": "text"
      },
      "source": [
        "让我们创建一个模型实例并对其进行训练。\n",
        "\n",
        "我们选择`sparse_categorical_crossentropy`作为模型的损失函数。模型的输出的形状为`[batch_size, 10]`。该模型的目标是一个整数向量，每个整数都在0到9的范围内。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYU_S2IefN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cd80e197-a66b-41e3-cb96-3e261205ec1f"
      },
      "source": [
        "model = build_model(allow_cudnn_kernel=True)\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 5s 5ms/step - loss: 0.9482 - accuracy: 0.7034 - val_loss: 0.4980 - val_accuracy: 0.8415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c2a0c86d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT2KKjUPegZa",
        "colab_type": "text"
      },
      "source": [
        "现在，让我们与不使用CuDNN内核的模型进行比较："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTrGxDE1ehze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "88f33c12-dbee-4208-94fb-e701aad06617"
      },
      "source": [
        "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
        "noncudnn_model.set_weights(model.get_weights())\n",
        "noncudnn_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "noncudnn_model.fit(\n",
        "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "938/938 [==============================] - 43s 45ms/step - loss: 0.3838 - accuracy: 0.8857 - val_loss: 0.4986 - val_accuracy: 0.8259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c2365eb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLYvSKmiei1Z",
        "colab_type": "text"
      },
      "source": [
        "在安装了NVIDIA GPU和CuDNN的计算机上运行时，与使用常规TensorFlow内核的模型相比，使用CuDNN构建的模型的训练速度要快得多。\n",
        "\n",
        "启用了CuDNN的模型也可以用于仅有CPU的环境中进行预测，下面的[`tf.device`](https://www.tensorflow.org/api_docs/python/tf/device)只是强制闲置设备。如果没有可用的GPU，默认情况下该模型将在CPU上运行。\n",
        "\n",
        "你完全不必关心正在运行的硬件，这不是很酷的一件事儿吗？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy3EQhcGekk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "102ac8d7-3789-48b9-f5f6-0d9f15df4a4e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with tf.device(\"CPU:0\"):\n",
        "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
        "    cpu_model.set_weights(model.get_weights())\n",
        "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
        "    print(\n",
        "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
        "    )\n",
        "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted result is: [3], target result is: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnVRuchVemZa",
        "colab_type": "text"
      },
      "source": [
        "### **具有列表/字典输入或嵌套输入的RNN**\n",
        "\n",
        "嵌套结构使得实现者在单个时间步之内能够包括更多信息，例如，一个视频帧可以同时具有音频和视频输入。在这种情况下，数据形状可能是：\n",
        "\n",
        "`[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]`\n",
        "\n",
        "在另一个示例中，笔迹数据可以具有笔的当前位置的坐标x和y以及压力信息。因此，数据表示可以是：\n",
        "\n",
        "`[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]`\n",
        "\n",
        "以下代码提供了一个示例，展示如何构建接受此类结构化输入的自定义RNN单元。\n",
        "\n",
        "### **定义一个支持嵌套输入/输出的自定义单元**\n",
        "\n",
        "有关编写自己的层的详细信息，请参见[通过子类化创建新的层和模型](https://www.tensorflow.org/guide/keras/custom_layers_and_models/)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-51FNsceujq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NestedCell(keras.layers.Layer):\n",
        "    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\n",
        "        self.unit_1 = unit_1\n",
        "        self.unit_2 = unit_2\n",
        "        self.unit_3 = unit_3\n",
        "        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
        "        super(NestedCell, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        # input_shape应该包含2个条目，[（batch，i1），（batch，i2，i3）]\n",
        "        i1 = input_shapes[0][1]\n",
        "        i2 = input_shapes[1][1]\n",
        "        i3 = input_shapes[1][2]\n",
        "\n",
        "        self.kernel_1 = self.add_weight(\n",
        "            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\n",
        "        )\n",
        "        self.kernel_2_3 = self.add_weight(\n",
        "            shape=(i2, i3, self.unit_2, self.unit_3),\n",
        "            initializer=\"uniform\",\n",
        "            name=\"kernel_2_3\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # 输入的形状应为[(batch, input_1), (batch, input_2, input_3)]\n",
        "        # 状态的形状应为[(batch, unit_1), (batch, unit_2, unit_3)]\n",
        "        input_1, input_2 = tf.nest.flatten(inputs)\n",
        "        s1, s2 = states\n",
        "\n",
        "        output_1 = tf.matmul(input_1, self.kernel_1)\n",
        "        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\n",
        "        state_1 = s1 + output_1\n",
        "        state_2_3 = s2 + output_2_3\n",
        "\n",
        "        output = (output_1, output_2_3)\n",
        "        new_states = (state_1, state_2_3)\n",
        "\n",
        "        return output, new_states\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXyK7nC1ev0i",
        "colab_type": "text"
      },
      "source": [
        "#### **使用嵌套的输入/输出构建RNN模型**\n",
        "\n",
        "让我们使用[`keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)层和我们的自定义单元构建一个Keras模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8Ytj4riezGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_1 = 10\n",
        "unit_2 = 20\n",
        "unit_3 = 30\n",
        "\n",
        "i1 = 32\n",
        "i2 = 64\n",
        "i3 = 32\n",
        "batch_size = 64\n",
        "num_batches = 10\n",
        "timestep = 50\n",
        "\n",
        "cell = NestedCell(unit_1, unit_2, unit_3)\n",
        "rnn = keras.layers.RNN(cell)\n",
        "\n",
        "input_1 = keras.Input((None, i1))\n",
        "input_2 = keras.Input((None, i2, i3))\n",
        "\n",
        "outputs = rnn((input_1, input_2))\n",
        "\n",
        "model = keras.models.Model([input_1, input_2], outputs)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83uH2Iiae0ZY",
        "colab_type": "text"
      },
      "source": [
        "#### **使用随机生成的数据训练模型**\n",
        "\n",
        "由于此模型没有好的候选数据集，因此我们使用随机的Numpy数据进行演示。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_w9NJxre4Bj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7c8c0972-d082-46e0-c603-e01a7b0fe45f"
      },
      "source": [
        "input_1_data = np.random.random((batch_size * num_batches, timestep, i1))\n",
        "input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3))\n",
        "target_1_data = np.random.random((batch_size * num_batches, unit_1))\n",
        "target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3))\n",
        "input_data = [input_1_data, input_2_data]\n",
        "target_data = [target_1_data, target_2_data]\n",
        "\n",
        "model.fit(input_data, target_data, batch_size=batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 33ms/step - loss: 0.7470 - rnn_1_loss: 0.2562 - rnn_1_1_loss: 0.4908 - rnn_1_accuracy: 0.0953 - rnn_1_1_accuracy: 0.0366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c222d6470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBSoun_Ve5Pu",
        "colab_type": "text"
      },
      "source": [
        "使用[`keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)层，只需要为序列中的单个步骤定义数学逻辑，而[`keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN)层将为你处理序列迭代，这是一种快速原型化新型RNN（例如LSTM变体）的强大方法。\n",
        "\n",
        "有关更多详细信息，请访问[API文档](https://https//www.tensorflow.org/api_docs/python/tf/keras/layers/RNN/)。"
      ]
    }
  ]
}