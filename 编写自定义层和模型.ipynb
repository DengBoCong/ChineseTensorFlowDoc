{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "编写自定义层和模型.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrrrFgzrZLzLYWXI31hGSe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON9w2DEPm57O",
        "colab_type": "text"
      },
      "source": [
        "# **通过子类化定义新层和模型**\n",
        "\n",
        "### **引入**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtbT_27r2wO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPb3JeSpndgP",
        "colab_type": "text"
      },
      "source": [
        "### `Layer`类：状态（权重）和一些计算的组合体\n",
        "\n",
        "`Layer`类是Keras的核心抽象类之一。层封装了状态（层的“权重”）和从输入到输出的转换（“调用”，即层的前向传递）。\n",
        "\n",
        "下面是一个密度连接的层。它具有一个状态：变量`w`和`b`。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA-cgHz8ppzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXXGwk8fszwg",
        "colab_type": "text"
      },
      "source": [
        "你可以通过在某些张量输入上调用层来使用它，就像使用Python函数一样。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soIpnlxwtPRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d4ee3e5d-06ac-4a4d-de1b-a7fb2c1dfd8c"
      },
      "source": [
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.03713623  0.08639161 -0.02649306 -0.05403492]\n",
            " [ 0.03713623  0.08639161 -0.02649306 -0.05403492]], shape=(2, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMCQ2nc6uayu",
        "colab_type": "text"
      },
      "source": [
        "请注意，权重`w`和`b`在设置为层的属性时会自动由层跟踪："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWGPjDOZupmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFA-ysi5ur_F",
        "colab_type": "text"
      },
      "source": [
        "请注意，你还可以使用一种更快的方法为层添加权重：`add_weight()`方法："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv152zmPu0bj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "996da62e-ac40-4217-c417-5a28072ee408"
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)\n",
        "print(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.07010797  0.00309221  0.00851233  0.04159135]\n",
            " [-0.07010797  0.00309221  0.00851233  0.04159135]], shape=(2, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxizdwlu_Oh",
        "colab_type": "text"
      },
      "source": [
        "### **层可以拥有不可训练的权重**\n",
        "\n",
        "除了可训练的权重之外，你还可以向层添加不可训练的权重。在训练层时，不能再反向传播期间使用这类权重。\n",
        "\n",
        "以下是添加和使用不可训练的权重的方法："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdpQtVGpva2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d9df2a3b-3e53-471c-b8da-7ce2dc836b4b"
      },
      "source": [
        "class ComputeSum(keras.layers.Layer):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ComputeSum, self).__init__()\n",
        "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "        return self.total\n",
        "\n",
        "\n",
        "x = tf.ones((2, 2))\n",
        "my_sum = ComputeSum(2)\n",
        "y = my_sum(x)\n",
        "print(y.numpy())\n",
        "y = my_sum(x)\n",
        "print(y.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UemSCCkFwBlK",
        "colab_type": "text"
      },
      "source": [
        "它是`layer.weights`的一部分，但被归类为不可训练的权重："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o5XZ-JWwIQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6b391b27-a217-42f8-de8f-8883de9e241d"
      },
      "source": [
        "print(\"weights:\", len(my_sum.weights))\n",
        "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
        "\n",
        "# 不包含可训练权重:\n",
        "print(\"trainable_weights:\", my_sum.trainable_weights)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights: 1\n",
            "non-trainable weights: 1\n",
            "trainable_weights: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI2ws_h9x8HA",
        "colab_type": "text"
      },
      "source": [
        "### 最佳实践：得到输入的形状后，再创建权重\n",
        "\n",
        "我们上面的`Linear`层采用了`input_dim`参数，用于计算`__init__()`中权重`w`和`b`的形状："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVPgvH8ByaWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhc50mNvyeoP",
        "colab_type": "text"
      },
      "source": [
        "在许多情况下，你可能事先不知道输入的大小，并且你想在实例化层后的某个时间，得到输入大小后再创建权重。\n",
        "\n",
        "在Keras API中，建议你在层的`build(self，inputs_shape)`方法中创建层的权重。像如下示例这样："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmf3cMhj7uMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQXbhp4O77UK",
        "colab_type": "text"
      },
      "source": [
        "层的`__call__()`方法在首次调用时将自动执行`build`。现在你有了一个懒惰的层，因此更易于使用："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZLkZkw38T4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 实例化时，我们不知道使用何种输入进行调用\n",
        "linear_layer = Linear(32)\n",
        "\n",
        "# 首次调用该层时，将动态创建该层的权重\n",
        "y = linear_layer(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEV9B_VmVV5_",
        "colab_type": "text"
      },
      "source": [
        "### **层可以递归组合**\n",
        "\n",
        "如果将层实例作为另一个层的属性，则外层将会开始跟踪内层的权重。\n",
        "\n",
        "我们建议在`__init__()`方法中创建这样的子层（由于子层通常具有构建方法，因此将在外层构建时构建它们）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7K1RrRcWTjV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3d00d81f-cfe7-480e-9c91-4f4e55573e36"
      },
      "source": [
        "# 我们将上面定义了“build”方法的Linear类一起使用。\n",
        "\n",
        "\n",
        "class MLPBlock(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(MLPBlock, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "\n",
        "mlp = MLPBlock()\n",
        "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
        "print(\"weights:\", len(mlp.weights))\n",
        "print(\"trainable weights:\", len(mlp.trainable_weights))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights: 6\n",
            "trainable weights: 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLZmlqNyW07v",
        "colab_type": "text"
      },
      "source": [
        "### **add_loss()方法**\n",
        "\n",
        "编写层的`call()`方法时，可以创建损失张量，随后将在编写训练循环时使用。这可以通过调用`self.add_loss(value)`来实现："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXidThTyXI7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 产生活动正则化损失的层\n",
        "class ActivityRegularizationLayer(keras.layers.Layer):\n",
        "    def __init__(self, rate=1e-2):\n",
        "        super(ActivityRegularizationLayer, self).__init__()\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "        return inputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2enzuUkqXaHY",
        "colab_type": "text"
      },
      "source": [
        "这些损失（包括由所有内部层产生的损失）可以通过`layer.losses`进行检索。此属性在顶层的`__call__()`开始时重置，因此`layer.losses`始终包含在上一次正向传递过程中创建的损失值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ePmpi66YfJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OuterLayer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayer, self).__init__()\n",
        "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activity_reg(inputs)\n",
        "\n",
        "\n",
        "layer = OuterLayer()\n",
        "assert len(layer.losses) == 0  # 由于从未调用过该层，因此尚未产生任何损失\n",
        "\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # 创建一个损失值\n",
        "\n",
        "# `layer.losses`在__call__开始时被重置\n",
        "_ = layer(tf.zeros(1, 1))\n",
        "assert len(layer.losses) == 1  # 这是上面调用中造成的损失"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i48qixC7Z8HQ",
        "colab_type": "text"
      },
      "source": [
        "此外，`loss`属性还包含为所有内层的权重创建的正则化损失："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nXwo8OfaEO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa01f64f-df30-4e64-9fd1-af45e629a77b"
      },
      "source": [
        "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
        "        self.dense = keras.layers.Dense(\n",
        "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "\n",
        "layer = OuterLayerWithKernelRegularizer()\n",
        "_ = layer(tf.zeros((1, 1)))\n",
        "\n",
        "# 下面的值由`1e-3 * sum(layer.dense.kernel ** 2)`计算得来,\n",
        "# 即通过上面的`kernel_regularizer`创建的.\n",
        "print(layer.losses)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0022100222>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3OQa16Raybi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "在编写训练循环时应考虑这些损失，如下所示："
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OV1cWo7a1In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 实例化一个优化器\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 遍历数据集的批量。\n",
        "for x_batch_train, y_batch_train in train_dataset:\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = layer(x_batch_train)  # 小批量的Logits\n",
        "    # 小批量的损失值\n",
        "    loss_value = loss_fn(y_batch_train, logits)\n",
        "    # 添加在前向传递中产生的额外损失：\n",
        "    loss_value += sum(model.losses)\n",
        "\n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfWyOIJ-b6fj",
        "colab_type": "text"
      },
      "source": [
        "有关编写训练循环的详细指南，请参阅[从头开始编写训练循环的指南](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/)。\n",
        "\n",
        "这些损失还可以通过`fit()`无缝工作（它们会自动求和并添加到主要损失中）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aeR2ytbcTQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7b8f4604-d3a1-46fb-f303-13677690afd0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "outputs = ActivityRegularizationLayer()(inputs)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# 如果在`compile`中传递了损失，则将正则化损失添加到其中\n",
        "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
        "\n",
        "# 也可以不给`compile`传递损失，因为模型已经在前向传递过程中调用`add_loss`使损失最小化了！\n",
        "model.compile(optimizer=\"adam\")\n",
        "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1177\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4ed003e630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiRwBX-neHj3",
        "colab_type": "text"
      },
      "source": [
        "### **add_metric()方法**\n",
        "\n",
        "与`add_loss()`类似，层还具有`add_metric()`方法，用于在训练过程中跟踪指标的变化平均值。\n",
        "\n",
        "考虑以下的示例：“LogisticEndpoint”层。它以预测和目标作为输入，通过`add_loss()`跟踪计算的损失，并通过`add_metric()`跟踪计算的精度标量。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvgqzVH0f0ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticEndpoint(keras.layers.Layer):\n",
        "    def __init__(self, name=None):\n",
        "        super(LogisticEndpoint, self).__init__(name=name)\n",
        "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "    def call(self, targets, logits, sample_weights=None):\n",
        "        # 计算训练时的损失，并使用`self.add_loss()`将其添加到层\n",
        "        loss = self.loss_fn(targets, logits, sample_weights)\n",
        "        self.add_loss(loss)\n",
        "\n",
        "        # 记录精度指标，并使用`self.add_metric()`将其添加到层\n",
        "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
        "        self.add_metric(acc, name=\"accuracy\")\n",
        "\n",
        "        # 返回预测的张量(即`.predict()`).\n",
        "        return tf.nn.softmax(logits)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLfgL7UfgeT8",
        "colab_type": "text"
      },
      "source": [
        "可通过`layer.metrics`访问以这种方式跟踪的指标："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8PSNkY3eqSj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f12e9606-4892-4cfd-cc0f-169cb053227a"
      },
      "source": [
        "layer = LogisticEndpoint()\n",
        "\n",
        "targets = tf.ones((2, 2))\n",
        "logits = tf.ones((2, 2))\n",
        "y = layer(targets, logits)\n",
        "\n",
        "print(\"layer.metrics:\", layer.metrics)\n",
        "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer.metrics: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7f4ee1e140f0>]\n",
            "current accuracy value: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPdaGFpAg21U",
        "colab_type": "text"
      },
      "source": [
        "就像`add_loss()`一样，这些指标也可以通过`fit()`进行跟踪："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXV4Va1Pg9X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9c7c8f76-3107-4e84-b071-70f6037abc10"
      },
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
        "targets = keras.Input(shape=(10,), name=\"targets\")\n",
        "logits = keras.layers.Dense(10)(inputs)\n",
        "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
        "\n",
        "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
        "model.compile(optimizer=\"adam\")\n",
        "\n",
        "data = {\n",
        "    \"inputs\": np.random.random((3, 3)),\n",
        "    \"targets\": np.random.random((3, 10)),\n",
        "}\n",
        "model.fit(data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.9742 - binary_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4e86057278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdJkGTmhJuU",
        "colab_type": "text"
      },
      "source": [
        "### **你可以选择性的启用层的序列化**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEm5J-GghXCH",
        "colab_type": "text"
      },
      "source": [
        "如果需要将自定义层作为函数式模型的一部分进行序列化，则可以选择实现`get_config()`方法："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG6tkSP1hlcG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2819887f-51f0-4970-f6c4-d0e7407f982f"
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"units\": self.units}\n",
        "\n",
        "\n",
        "# 现在，你可以通过from_config重新创建该层：\n",
        "layer = Linear(64)\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'units': 64}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnmXXdhliOC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8772d5f8-0d54-45d4-fcce-a71ee4393e78"
      },
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(Linear, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Linear, self).get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n",
        "\n",
        "\n",
        "layer = Linear(64)\n",
        "config = layer.get_config()\n",
        "print(config)\n",
        "new_layer = Linear.from_config(config)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQMozd5ib-1",
        "colab_type": "text"
      },
      "source": [
        "如果需要更大的灵活性，从其配置反序列化层时，则可以重写`from_config()`类方法来实现。下面是`from_config()`的基本实现："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2ABPWq9jPEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def from_config(cls, config):\n",
        "  return cls(**config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eHonr-BjSAH",
        "colab_type": "text"
      },
      "source": [
        "要了解有关序列化和保存的更多信息，请参阅有关[模型保存和序列化的完整指南](https://www.tensorflow.org/guide/keras/save_and_serialize/)。\n",
        "\n",
        "### **call()方法中特殊的训练参数**\n",
        "\n",
        "某些层，尤其是`BatchNormalization`层和`Dropout`层，在训练和预测期间具有不同的行为。 对于此类层，标准做法是在`call()`方法中公开训练（boolean）参数。\n",
        "\n",
        "通过在`call()`中公开此参数，可以启用内置的训练和评估循环（例如`fit()`）以在训练和预测中正确使用该层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NEzpO5ij-BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDropout(keras.layers.Layer):\n",
        "    def __init__(self, rate, **kwargs):\n",
        "        super(CustomDropout, self).__init__(**kwargs)\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            return tf.nn.dropout(inputs, rate=self.rate)\n",
        "        return inputs"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvvhCnRHkMc8",
        "colab_type": "text"
      },
      "source": [
        "### **call()方法中特殊的掩码参数**\n",
        "\n",
        "`call()`支持的另一个特殊参数是`mask`参数。\n",
        "\n",
        "你可以在所有Keras RNN层中找到它。掩码是布尔张量（输入中每个时间步有一个布尔值），用于在处理时间序列数据时跳过某些输入时间步。\n",
        "\n",
        "当先前的层生成`mask`时，Keras会自动将正确的`mask`参数传递给支持该层的`__call__()`。`mask`生成层是配置有`mask_zero = True`的“`Embedding`”层和“`Masking`”层。\n",
        "\n",
        "要了解有关masking以及如何编写masking-enabled的层的更多信息，请查看指南“[理解padding和masking](https://www.tensorflow.org/guide/keras/masking_and_padding/)”。\n",
        "\n",
        "### **Model类**\n",
        "\n",
        "通常情况下，你将使用`Layer`类来定义内部计算块，并使用`Model`类来定义外部模型--你将训练的对象。\n",
        "\n",
        "例如，在**ResNet50**模型中，你将有几个子类化`Layer`的**ResNet**，以及一个`Model`则包含了整个**ResNet50**网络。\n",
        "\n",
        "`Model`类具有与`Layer`同样的API，但有以下区别：\n",
        "+ 它公开了内置的训练，评估和预测循环（`model.fit()`，`model.evaluate()`，`model.predict()`）。\n",
        "+ 它通过`model.layers`属性公开其内层的列表。\n",
        "+ 它公开了保存和序列化API（`save()`，`save_weights()`...）\n",
        "\n",
        "实际上，`Layer`类对应于我们在文献中所称的“层”（如“卷积层”或“递归层”）或“块”（如“ ResNet块”或“ Inception块”） ）。\n",
        "\n",
        "同时，`Model`类对应于文献中所谓的“模型”（如“深度学习模型”）或“网络”（如“深度神经网络”）。\n",
        "\n",
        "因此，如果你想知道“我应该使用`Layer`类还是`Model`类？”，请问自己：我需要在其上调用`fit()`吗？我需要在上面调用`save()`吗？如果需要，请选择`Model`。如果不需要（因为你的类只是更大系统中的一个块，或者是因为你自己编写训练和保存代码），请使用`Layer`。\n",
        "\n",
        "例如，我们可以以上面的mini-resnet示例为例，并使用它来构建一个模型，该模型可以通过`fit()`进行训练，并可以通过`save_weights()`保存："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz4GDDp-qDb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.block_1 = ResNetBlock()\n",
        "        self.block_2 = ResNetBlock()\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.classifier = Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.block_1(inputs)\n",
        "        x = self.block_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "resnet = ResNet()\n",
        "dataset = ...\n",
        "resnet.fit(dataset, epochs=10)\n",
        "resnet.save(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roF8dDCcqPW1",
        "colab_type": "text"
      },
      "source": [
        "### 整合所有知识点：端到端的示例\n",
        "\n",
        "+ 到目前为止，你已经学到了以下内容：\n",
        "+ `Layer`封装状态（在`__init__()`或`build()`中创建）和一些计算（在`call()`中定义）\n",
        "+ 可以递归嵌套层以创建更大的新计算块\n",
        "+ 层可以通过`add_loss()`和`add_metric()`创建和跟踪损失（通常是正则化损失）以及指标\n",
        "+ 你要训练的外部容器是一个`Model`。模型就像一个`Layer`，但是增加了训练和序列化功能\n",
        "\n",
        "让我们将所有这些内容放到一个端到端的示例中：我们将实现一个变体自动编码器（VAE）。我们将用MNIST数字对其进行训练。\n",
        "\n",
        "我们的VAE将是`Model`的子类，通过`Layer`的子类嵌套构建的。它将具有正则化损失（KL散度）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71W4AQ6TrlW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"使用(z_mean，z_log_var)采样z，该向量编码一个数字。”\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    \"\"\"将MNIST数字映射到三元组(z_mean，z_log_var，z)\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "        self.sampling = Sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        z_mean = self.dense_mean(x)\n",
        "        z_log_var = self.dense_log_var(x)\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    \"\"\"将编码的数字矢量z转换回可读的数字\"\"\"\n",
        "\n",
        "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
        "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        return self.dense_output(x)\n",
        "\n",
        "\n",
        "class VariationalAutoEncoder(keras.Model):\n",
        "    \"\"\"将编码器和解码器整合到端到端模型以进行训练\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_dim,\n",
        "        intermediate_dim=64,\n",
        "        latent_dim=32,\n",
        "        name=\"autoencoder\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
        "        self.original_dim = original_dim\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        # 添加KL散度作为正则化损失.\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        "        )\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDqwhZ5fv0ne",
        "colab_type": "text"
      },
      "source": [
        "让我们在MNIST上编写一个简单的训练循环："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF5bdL9bv5t1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "65f1a996-1585-4143-f1ed-f2856519ad96"
      },
      "source": [
        "original_dim = 784\n",
        "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "# 遍历epochs.\n",
        "for epoch in range(epochs):\n",
        "    print(\"Start of epoch %d\" % (epoch,))\n",
        "\n",
        "    # 遍历dataset的批量\n",
        "    for step, x_batch_train in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed = vae(x_batch_train)\n",
        "            # 计算重建损失\n",
        "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "            loss += sum(vae.losses)  # 添加KLD正则化\n",
        "        grads = tape.gradient(loss, vae.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "        loss_metric(loss)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Start of epoch 0\n",
            "step 0: mean loss = 0.3072\n",
            "step 100: mean loss = 0.1247\n",
            "step 200: mean loss = 0.0988\n",
            "step 300: mean loss = 0.0889\n",
            "step 400: mean loss = 0.0840\n",
            "step 500: mean loss = 0.0807\n",
            "step 600: mean loss = 0.0786\n",
            "step 700: mean loss = 0.0770\n",
            "step 800: mean loss = 0.0759\n",
            "step 900: mean loss = 0.0749\n",
            "Start of epoch 1\n",
            "step 0: mean loss = 0.0746\n",
            "step 100: mean loss = 0.0739\n",
            "step 200: mean loss = 0.0734\n",
            "step 300: mean loss = 0.0730\n",
            "step 400: mean loss = 0.0726\n",
            "step 500: mean loss = 0.0723\n",
            "step 600: mean loss = 0.0719\n",
            "step 700: mean loss = 0.0717\n",
            "step 800: mean loss = 0.0714\n",
            "step 900: mean loss = 0.0712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im-_tYK4w9AN",
        "colab_type": "text"
      },
      "source": [
        "请注意，由于VAE是`Model`的子类，所以它具有内置的训练循环。因此，你也可以像这样训练它："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0EhlyDIxFb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "06f95afd-fc7b-40f9-c54c-a8ec17464065"
      },
      "source": [
        "vae = VariationalAutoEncoder(784, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0747\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4e762f5550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7iD5QiDxMPP",
        "colab_type": "text"
      },
      "source": [
        "### **超越面向对象的开发：函数式API**\n",
        "\n",
        "前面的示例对你来说，是不是使用了太多的面向对象开发？你也可以使用函数式API构建模型。重要的是，不论选择哪种风格不会妨碍你利用以另一种风格编写的组件：你始终可以混合搭配使用。\n",
        "\n",
        "例如，下面的函数式API示例中，重用了我们在上面示例中定义的相同`Sampling`层："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwRGeCJyCCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7a8f0789-dfa6-4976-bf32-d85adf065012"
      },
      "source": [
        "original_dim = 784\n",
        "intermediate_dim = 64\n",
        "latent_dim = 32\n",
        "\n",
        "# 定义编码模型\n",
        "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()((z_mean, z_log_var))\n",
        "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
        "\n",
        "# 定义解码模型\n",
        "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
        "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
        "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
        "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
        "\n",
        "# 定义VAE模型\n",
        "outputs = decoder(z)\n",
        "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
        "\n",
        "# 添加KL散度为正则化损失\n",
        "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "vae.add_loss(kl_loss)\n",
        "\n",
        "# 训练\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0746\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0677\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.0676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4e761431d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfPAt3qzyF_5",
        "colab_type": "text"
      },
      "source": [
        "有关更多信息，请确保阅读[函数式API指南](https://www.tensorflow.org/guide/keras/functional/)。"
      ]
    }
  ]
}