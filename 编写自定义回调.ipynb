{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "编写自定义回调.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTRJJZtfRZ+HG4keLtuoXo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAYwLOBYHKb",
        "colab_type": "text"
      },
      "source": [
        "# **编写自定义回调**\n",
        "\n",
        "### **介绍**\n",
        "\n",
        "回调是一种强大的工具，可以在训练，评估或预测期间自定义Keras模型的行为。示例包括通过TensorBoard的[`tf.keras.callbacks.TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard)可视化训练进度和结果，或[`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)在训练期间定期保存模型。\n",
        "\n",
        "在本指南中，你将了解Keras回调是什么，它可以做什么以及如何构建自己的回调。我们提供了一些简单的回调示例，以帮助你入门。\n",
        "\n",
        "### **引入**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tivL9jQfX_Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUlCZBMfYTlt",
        "colab_type": "text"
      },
      "source": [
        "### **Keras回调概述**\n",
        "\n",
        "所有回调都是`keras.callbacks.Callback`类的子类，并覆盖在训练，评估和预测的各个阶段调用的一组方法。回调对于在训练期间了解模型的内部状态和统计信息很有用。\n",
        "\n",
        "你可以将回调列表（使用关键字参数`callbacks`）传递给以下模型方法：\n",
        "\n",
        "+ [`keras.Model.fit()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
        "+ [`keras.Model.evaluate()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate)\n",
        "+ [`keras.Model.predict()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict)\n",
        "\n",
        "### **回调方法概述**\n",
        "#### **全局方法**\n",
        "\n",
        "`on_(train|test|predict)_begin(self, logs=None)`\n",
        "\n",
        "在`fit`/`evaluate`/`predict`开始时调用。\n",
        "\n",
        "`on_(train|test|predict)_end(self, logs=None)`\n",
        "在`fit`/`evaluate`/`predict`结束时调用。\n",
        "\n",
        "#### **用于训练/验证/预测的批处理方法**\n",
        "\n",
        "`on_(train|test|predict)_batch_begin(self, batch, logs=None) `\n",
        "\n",
        "在训练/验证/预测期间，处理批次之前立即调用。\n",
        "\n",
        "`on_(train|test|predict)_batch_end(self, batch, logs=None)`\n",
        "\n",
        "在训练/验证/预测批次结束时调用，在此方法中， logs是包含指标结果的字典。\n",
        "\n",
        "#### **epoch级别的方法（仅训练）**\n",
        "\n",
        "`on_epoch_begin(self, epoch, logs=None)`\n",
        "\n",
        "在训练期间的epoch开始时调用。\n",
        "\n",
        "`on_epoch_end(self, epoch, logs=None)`\n",
        "\n",
        "在训练期间的epoch末尾调用。\n",
        "\n",
        "### **一个基本的示例**\n",
        "\n",
        "让我们看一个具体的例子，首先，让我们导入tensorflow并定义一个简单的Sequential模型："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjda5RBFYr_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义Keras模型用于添加回调\n",
        "def get_model():\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(1, input_dim=784))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
        "        loss=\"mean_squared_error\",\n",
        "        metrics=[\"mean_absolute_error\"],\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPgyTSDjYs-v",
        "colab_type": "text"
      },
      "source": [
        "然后，从Keras数据集API中加载MNIST数据用于训练和测试："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOtj1XaPYuHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5a7e6e7b-f285-4aae-d5aa-1d511334d29f"
      },
      "source": [
        "# 加载MNIST数据并进行预处理\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
        "\n",
        "# 仅使用1000个样本\n",
        "x_train = x_train[:1000]\n",
        "y_train = y_train[:1000]\n",
        "x_test = x_test[:1000]\n",
        "y_test = y_test[:1000]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO9B5cmmY11F",
        "colab_type": "text"
      },
      "source": [
        "现在，定义一个简单的自定义回调来记录：\n",
        "\n",
        "+ `fit`/`evaluate`/`predict`何时开始和结束\n",
        "+ 每个epoch何时开始和结束\n",
        "+ 每批训练何时开始和结束\n",
        "+ 每个评估（测试）批量何时开始和结束\n",
        "+ 每个推断（预测）批量何时开始和结束"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I0I6EE_Y4Eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmsx0CzoY46q",
        "colab_type": "text"
      },
      "source": [
        "让我们尝试运行一下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhBCWZPqY56N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4d20d3b-51db-4748-b0f9-ef9fa6e8cb28"
      },
      "source": [
        "model = get_model()\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    verbose=0,\n",
        "    validation_split=0.5,\n",
        "    callbacks=[CustomCallback()],\n",
        ")\n",
        "\n",
        "res = model.evaluate(\n",
        "    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n",
        ")\n",
        "\n",
        "res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training; got log keys: []\n",
            "Start epoch 0 of training; got log keys: []\n",
            "...Training: start of batch 0; got log keys: []\n",
            "...Training: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Training: start of batch 1; got log keys: []\n",
            "...Training: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Training: start of batch 2; got log keys: []\n",
            "...Training: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Training: start of batch 3; got log keys: []\n",
            "...Training: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
            "Start testing; got log keys: []\n",
            "...Evaluating: start of batch 0; got log keys: []\n",
            "...Evaluating: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 1; got log keys: []\n",
            "...Evaluating: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 2; got log keys: []\n",
            "...Evaluating: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 3; got log keys: []\n",
            "...Evaluating: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
            "Stop testing; got log keys: ['loss', 'mean_absolute_error']\n",
            "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error']\n",
            "Stop training; got log keys: ['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error']\n",
            "Start testing; got log keys: []\n",
            "...Evaluating: start of batch 0; got log keys: []\n",
            "...Evaluating: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 1; got log keys: []\n",
            "...Evaluating: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 2; got log keys: []\n",
            "...Evaluating: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 3; got log keys: []\n",
            "...Evaluating: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 4; got log keys: []\n",
            "...Evaluating: end of batch 4; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 5; got log keys: []\n",
            "...Evaluating: end of batch 5; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 6; got log keys: []\n",
            "...Evaluating: end of batch 6; got log keys: ['loss', 'mean_absolute_error']\n",
            "...Evaluating: start of batch 7; got log keys: []\n",
            "...Evaluating: end of batch 7; got log keys: ['loss', 'mean_absolute_error']\n",
            "Stop testing; got log keys: ['loss', 'mean_absolute_error']\n",
            "Start predicting; got log keys: []\n",
            "...Predicting: start of batch 0; got log keys: []\n",
            "...Predicting: end of batch 0; got log keys: ['outputs']\n",
            "...Predicting: start of batch 1; got log keys: []\n",
            "...Predicting: end of batch 1; got log keys: ['outputs']\n",
            "...Predicting: start of batch 2; got log keys: []\n",
            "...Predicting: end of batch 2; got log keys: ['outputs']\n",
            "...Predicting: start of batch 3; got log keys: []\n",
            "...Predicting: end of batch 3; got log keys: ['outputs']\n",
            "...Predicting: start of batch 4; got log keys: []\n",
            "...Predicting: end of batch 4; got log keys: ['outputs']\n",
            "...Predicting: start of batch 5; got log keys: []\n",
            "...Predicting: end of batch 5; got log keys: ['outputs']\n",
            "...Predicting: start of batch 6; got log keys: []\n",
            "...Predicting: end of batch 6; got log keys: ['outputs']\n",
            "...Predicting: start of batch 7; got log keys: []\n",
            "...Predicting: end of batch 7; got log keys: ['outputs']\n",
            "Stop predicting; got log keys: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5fAmCzHY7Jk",
        "colab_type": "text"
      },
      "source": [
        "### **logs字典的用法**\n",
        "\n",
        "`logs`字典包含损失值以及批量或epoch末尾的所有指标，示例包括损失和平均绝对误差。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkRHOygbY_M5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "293ca1ba-f8ea-445d-8c7d-2f0f206c2a16"
      },
      "source": [
        "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\n",
        "            \"The average loss for epoch {} is {:7.2f} \"\n",
        "            \"and mean absolute error is {:7.2f}.\".format(\n",
        "                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    verbose=0,\n",
        "    callbacks=[LossAndErrorPrintingCallback()],\n",
        ")\n",
        "\n",
        "res = model.evaluate(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    batch_size=128,\n",
        "    verbose=0,\n",
        "    callbacks=[LossAndErrorPrintingCallback()],\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For batch 0, loss is   26.27.\n",
            "For batch 1, loss is  427.43.\n",
            "For batch 2, loss is  293.28.\n",
            "For batch 3, loss is  222.45.\n",
            "For batch 4, loss is  179.59.\n",
            "For batch 5, loss is  150.72.\n",
            "For batch 6, loss is  130.03.\n",
            "For batch 7, loss is  116.93.\n",
            "The average loss for epoch 0 is  116.93 and mean absolute error is    5.83.\n",
            "For batch 0, loss is    4.09.\n",
            "For batch 1, loss is    4.54.\n",
            "For batch 2, loss is    4.57.\n",
            "For batch 3, loss is    4.86.\n",
            "For batch 4, loss is    4.92.\n",
            "For batch 5, loss is    4.72.\n",
            "For batch 6, loss is    4.50.\n",
            "For batch 7, loss is    4.49.\n",
            "The average loss for epoch 1 is    4.49 and mean absolute error is    1.72.\n",
            "For batch 0, loss is    5.53.\n",
            "For batch 1, loss is    5.03.\n",
            "For batch 2, loss is    4.90.\n",
            "For batch 3, loss is    4.87.\n",
            "For batch 4, loss is    4.97.\n",
            "For batch 5, loss is    5.02.\n",
            "For batch 6, loss is    4.90.\n",
            "For batch 7, loss is    4.83.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHii8xrZCqr",
        "colab_type": "text"
      },
      "source": [
        "### **self.model属性的用法**\n",
        "\n",
        "除了在调用其中一种方法时接收日志信息外，回调还可以访问与当前一轮训练/评估/预测相关的模型： `self.model`。\n",
        "\n",
        "以下是你可以在回调中使用`self.model`进行的一些操作：\n",
        "\n",
        "+ 设置`self.model.stop_training = True`可以立即中断训练。\n",
        "+ 修改优化器的超参数（当`self.model.optimizer`可用时），例如`self.model.optimizer.learning_rate`。\n",
        "+ 定期保存模型。\n",
        "+ 在每个epoch结束时，在一些测试样本上记录`model.predict()`的输出，用于在训练期间用作健壮性检查。\n",
        "+ 在每个epoch结束时提取中间特征的可视化，以监视模型随时间推移的正在学习的内容。\n",
        "+ 等等\n",
        "\n",
        "让我们在几个示例了解上述操作。\n",
        "\n",
        "### **Keras回调应用示例**\n",
        "\n",
        "#### **在最小的损失下尽早停止**\n",
        "\n",
        "第一个示例展示了在达到最小损失时，如何通过`Callback`设置属性`self.model.stop_training`（boolean）来停止训练。（可选）你可以提供一个参数`patience`来指定在达到局部最小值后，我们应该在停止之前等待多少个epoch。\n",
        "\n",
        "[`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)提供了更完整和通用的实现。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XJSpN8RZM5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "97f8bd31-63d8-4a49-f60c-c7530ba0ed14"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
        "    \"\"\"当损失达到最小时停止训练，即损失停止减少\n",
        "\n",
        "  参数:\n",
        "      patience: 达到最小后要等待的epoch数。在此数目没有改善后，训练将停止。\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, patience=0):\n",
        "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
        "        self.patience = patience\n",
        "        # best_weights用于存储发生最小损失的权重\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # 损失不再最小时已等待的epoch数\n",
        "        self.wait = 0\n",
        "        # 训练停止的epoch\n",
        "        self.stopped_epoch = 0\n",
        "        # 将最佳状态初始化为无穷大\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get(\"loss\")\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # 如果当前结果更好，则记录最佳权重\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    steps_per_epoch=5,\n",
        "    epochs=30,\n",
        "    verbose=0,\n",
        "    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()],\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For batch 0, loss is   33.73.\n",
            "For batch 1, loss is  435.45.\n",
            "For batch 2, loss is  297.71.\n",
            "For batch 3, loss is  226.21.\n",
            "For batch 4, loss is  182.16.\n",
            "The average loss for epoch 0 is  182.16 and mean absolute error is    8.19.\n",
            "For batch 0, loss is    8.40.\n",
            "For batch 1, loss is    7.32.\n",
            "For batch 2, loss is    6.56.\n",
            "For batch 3, loss is    6.03.\n",
            "For batch 4, loss is    5.64.\n",
            "The average loss for epoch 1 is    5.64 and mean absolute error is    1.92.\n",
            "For batch 0, loss is    6.03.\n",
            "For batch 1, loss is    5.28.\n",
            "For batch 2, loss is    5.47.\n",
            "For batch 3, loss is    5.76.\n",
            "For batch 4, loss is    6.27.\n",
            "The average loss for epoch 2 is    6.27 and mean absolute error is    2.05.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00003: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c289c5240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dWyHcx8ZOn3",
        "colab_type": "text"
      },
      "source": [
        "#### **学习率策略**\n",
        "\n",
        "在此示例中，我们展示了在训练过程中如何使用自定义的回调来动态更改优化器的学习率。\n",
        "\n",
        "有关更一般的实现，请参见[`callbacks.LearningRateScheduler`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzYdcNZgZR8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fab59976-ba7a-40a5-acb6-4e6439c5ac54"
      },
      "source": [
        "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
        "    \"\"\"学习率调度器，根据策略设置学习率\n",
        "\n",
        "  参数:\n",
        "      策略: 该函数以epoch索引（整数，从0开始的\n",
        "      索引）和当前学习率作为输入，并返回新的学习率作为输出（浮点数）\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, schedule):\n",
        "        super(CustomLearningRateScheduler, self).__init__()\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if not hasattr(self.model.optimizer, \"lr\"):\n",
        "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "        # 从模型的优化器获取当前学习率\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        # 调用策略功能以获取计划的学习率\n",
        "        scheduled_lr = self.schedule(epoch, lr)\n",
        "        # 在此epoch开始之前，将值设置回优化器\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
        "\n",
        "\n",
        "LR_SCHEDULE = [\n",
        "    # （开始epoch值，学习率）元组\n",
        "    (3, 0.05),\n",
        "    (6, 0.01),\n",
        "    (9, 0.005),\n",
        "    (12, 0.001),\n",
        "]\n",
        "\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    \"\"\"辅助方法可根据epoch检索计划的学习率\"\"\"\n",
        "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
        "        return lr\n",
        "    for i in range(len(LR_SCHEDULE)):\n",
        "        if epoch == LR_SCHEDULE[i][0]:\n",
        "            return LR_SCHEDULE[i][1]\n",
        "    return lr\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    steps_per_epoch=5,\n",
        "    epochs=15,\n",
        "    verbose=0,\n",
        "    callbacks=[\n",
        "        LossAndErrorPrintingCallback(),\n",
        "        CustomLearningRateScheduler(lr_schedule),\n",
        "    ],\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00000: Learning rate is 0.1000.\n",
            "For batch 0, loss is   25.36.\n",
            "For batch 1, loss is  459.60.\n",
            "For batch 2, loss is  312.96.\n",
            "For batch 3, loss is  237.00.\n",
            "For batch 4, loss is  191.14.\n",
            "The average loss for epoch 0 is  191.14 and mean absolute error is    8.20.\n",
            "\n",
            "Epoch 00001: Learning rate is 0.1000.\n",
            "For batch 0, loss is    6.35.\n",
            "For batch 1, loss is    6.04.\n",
            "For batch 2, loss is    6.23.\n",
            "For batch 3, loss is    5.88.\n",
            "For batch 4, loss is    5.90.\n",
            "The average loss for epoch 1 is    5.90 and mean absolute error is    1.99.\n",
            "\n",
            "Epoch 00002: Learning rate is 0.1000.\n",
            "For batch 0, loss is    5.19.\n",
            "For batch 1, loss is    6.58.\n",
            "For batch 2, loss is    7.74.\n",
            "For batch 3, loss is   10.32.\n",
            "For batch 4, loss is   12.51.\n",
            "The average loss for epoch 2 is   12.51 and mean absolute error is    3.00.\n",
            "\n",
            "Epoch 00003: Learning rate is 0.0500.\n",
            "For batch 0, loss is   23.88.\n",
            "For batch 1, loss is   11.89.\n",
            "For batch 2, loss is    8.87.\n",
            "For batch 3, loss is    7.67.\n",
            "For batch 4, loss is    7.05.\n",
            "The average loss for epoch 3 is    7.05 and mean absolute error is    2.08.\n",
            "\n",
            "Epoch 00004: Learning rate is 0.0500.\n",
            "For batch 0, loss is    3.25.\n",
            "For batch 1, loss is    4.26.\n",
            "For batch 2, loss is    4.28.\n",
            "For batch 3, loss is    4.12.\n",
            "For batch 4, loss is    4.37.\n",
            "The average loss for epoch 4 is    4.37 and mean absolute error is    1.66.\n",
            "\n",
            "Epoch 00005: Learning rate is 0.0500.\n",
            "For batch 0, loss is    4.40.\n",
            "For batch 1, loss is    4.23.\n",
            "For batch 2, loss is    4.03.\n",
            "For batch 3, loss is    4.12.\n",
            "For batch 4, loss is    4.04.\n",
            "The average loss for epoch 5 is    4.04 and mean absolute error is    1.62.\n",
            "\n",
            "Epoch 00006: Learning rate is 0.0100.\n",
            "For batch 0, loss is    3.25.\n",
            "For batch 1, loss is    3.65.\n",
            "For batch 2, loss is    3.66.\n",
            "For batch 3, loss is    3.89.\n",
            "For batch 4, loss is    4.03.\n",
            "The average loss for epoch 6 is    4.03 and mean absolute error is    1.63.\n",
            "\n",
            "Epoch 00007: Learning rate is 0.0100.\n",
            "For batch 0, loss is    5.37.\n",
            "For batch 1, loss is    4.55.\n",
            "For batch 2, loss is    3.93.\n",
            "For batch 3, loss is    3.76.\n",
            "For batch 4, loss is    3.70.\n",
            "The average loss for epoch 7 is    3.70 and mean absolute error is    1.45.\n",
            "\n",
            "Epoch 00008: Learning rate is 0.0100.\n",
            "For batch 0, loss is    2.33.\n",
            "For batch 1, loss is    2.85.\n",
            "For batch 2, loss is    2.89.\n",
            "For batch 3, loss is    2.84.\n",
            "For batch 4, loss is    3.05.\n",
            "The average loss for epoch 8 is    3.05 and mean absolute error is    1.38.\n",
            "\n",
            "Epoch 00009: Learning rate is 0.0050.\n",
            "For batch 0, loss is    2.94.\n",
            "For batch 1, loss is    3.22.\n",
            "For batch 2, loss is    3.26.\n",
            "For batch 3, loss is    3.44.\n",
            "For batch 4, loss is    3.33.\n",
            "The average loss for epoch 9 is    3.33 and mean absolute error is    1.47.\n",
            "\n",
            "Epoch 00010: Learning rate is 0.0050.\n",
            "For batch 0, loss is    3.74.\n",
            "For batch 1, loss is    3.47.\n",
            "For batch 2, loss is    3.59.\n",
            "For batch 3, loss is    3.74.\n",
            "For batch 4, loss is    3.72.\n",
            "The average loss for epoch 10 is    3.72 and mean absolute error is    1.49.\n",
            "\n",
            "Epoch 00011: Learning rate is 0.0050.\n",
            "For batch 0, loss is    4.13.\n",
            "For batch 1, loss is    4.04.\n",
            "For batch 2, loss is    3.47.\n",
            "For batch 3, loss is    3.39.\n",
            "For batch 4, loss is    3.41.\n",
            "The average loss for epoch 11 is    3.41 and mean absolute error is    1.46.\n",
            "\n",
            "Epoch 00012: Learning rate is 0.0010.\n",
            "For batch 0, loss is    3.28.\n",
            "For batch 1, loss is    2.52.\n",
            "For batch 2, loss is    2.80.\n",
            "For batch 3, loss is    2.64.\n",
            "For batch 4, loss is    2.65.\n",
            "The average loss for epoch 12 is    2.65 and mean absolute error is    1.30.\n",
            "\n",
            "Epoch 00013: Learning rate is 0.0010.\n",
            "For batch 0, loss is    3.78.\n",
            "For batch 1, loss is    3.55.\n",
            "For batch 2, loss is    3.62.\n",
            "For batch 3, loss is    3.71.\n",
            "For batch 4, loss is    3.64.\n",
            "The average loss for epoch 13 is    3.64 and mean absolute error is    1.48.\n",
            "\n",
            "Epoch 00014: Learning rate is 0.0010.\n",
            "For batch 0, loss is    2.24.\n",
            "For batch 1, loss is    2.93.\n",
            "For batch 2, loss is    2.81.\n",
            "For batch 3, loss is    3.06.\n",
            "For batch 4, loss is    3.20.\n",
            "The average loss for epoch 14 is    3.20 and mean absolute error is    1.36.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5c2705ef98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-CvbDU6Zbnt",
        "colab_type": "text"
      },
      "source": [
        "#### **内置Keras回调**\n",
        "\n",
        "请务必阅读[API文档](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/)，以检查现有的Keras回调。应用包括记录到CSV，保存模型，在TensorBoard中可视化指标等等！"
      ]
    }
  ]
}