{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "从头开始编写训练循环.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5P40Qw7J4kqBLxxNJq3VP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MyEWtBtYiQA",
        "colab_type": "text"
      },
      "source": [
        "# **从头开始编写训练循环**\n",
        "\n",
        "### **引入**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "867B__aVX9C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR7NZmvbYqKa",
        "colab_type": "text"
      },
      "source": [
        "### **介绍**\n",
        "\n",
        "Keras提供了默认的训练和评估循环，`fit()`和`evaluate()`。使用[内置方法的训练和评估指南](https://www.tensorflow.org/guide/keras/train_and_evaluate/)涵盖了它们的用法。\n",
        "\n",
        "如果你想自定义模型的学习算法，同时又想利用`fit()`的便利性（例如，使用`fit()`训练GAN），则可以子类化`Model`类并实现自己的`train_step()`方法，`train_step()`方法将在`fit()`中被重复调用。更多信息，可以前往[自定义fit()中发生的操作](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/)指南。\n",
        "\n",
        "现在，如果你想对训练和评估进行非常底层的控制，则应该从头开始编写自己的训练和评估循环，这就是本指南的内容。\n",
        "\n",
        "### **使用GradientTape：第一个端到端示例**\n",
        "\n",
        "在`GradientTape`范围内调用模型，可以使得你能够检索层的可训练权重相对于损失值的梯度。通过优化器实例，你可以使用这些梯度来更新这些变量（可以使用`model.trainable_weights`进行检索）。\n",
        "\n",
        "让我们思考一个简单的MNIST模型："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXMAjqX0dhRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqIwiUmdkGK",
        "colab_type": "text"
      },
      "source": [
        "让我们使用带有自定义训练循环的小批量梯度对其进行训练。\n",
        "\n",
        "首先，我们需要一个优化器，一个损失函数和一个数据集："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8yld5NudmK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d8be87c5-c319-4c02-e592-87ba80a5a587"
      },
      "source": [
        " # 实例化优化器\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# 实例化损失函数\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 准备训练数据\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_train, (-1, 784))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trcniHhqdnsk",
        "colab_type": "text"
      },
      "source": [
        "接下来是我们的训练循环：\n",
        "\n",
        "+ 我们使用`for`循环来迭代epoch\n",
        "+ 对于每个epoch，我们使用`for`循环来分批迭代数据集\n",
        "+ 对于每个批量，我们都打开一个`GradientTape()`域\n",
        "+ 在此域内，我们调用模型（前向传递）并计算损失\n",
        "+ 在域外，我们获取有关损失的模型权重的梯度\n",
        "+ 最后，我们使用优化器根据梯度更新模型的权重"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVDsquR6dpoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "de191ac1-815e-41e1-88a0-136ea4417c69"
      },
      "source": [
        " epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # 遍历数据集的批量\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        # 打开GradientTape记录在前向传递过程中运行的操作，这可以实现自动区分\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # 前向传递运行层\n",
        "            # 层应用于其输入的操作将记录在GradientTape上\n",
        "            logits = model(x_batch_train, training=True)  # 最小批量的Logits\n",
        "\n",
        "            # 计算该批量的损失值\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # 使用gradientTape自动检索可训练变量相对于损失的梯度。\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # 通过更新变量值以最小化损失，来完成梯度下降\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # 每200批量记录一次\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 70.8490\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 1.5749\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.8477\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.6865\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.7668\n",
            "Seen so far: 51264 samples\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.6792\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.7830\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.8290\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.7765\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.5991\n",
            "Seen so far: 51264 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds6Saqe2dq3w",
        "colab_type": "text"
      },
      "source": [
        "### **用低级方法处理指标**\n",
        "\n",
        "让我们在上面的基本循环中添加指标监控。\n",
        "\n",
        "你可以在从头开始编写的训练循环中，随时使用内置指标（或编写的自定义指标），流程如下：\n",
        "\n",
        "+ 在循环开始时实例化指标\n",
        "+ 在每个批量之后调用`metric.update_state()`\n",
        "+ 当需要显示指标的当前值时，调用`metric.result()`\n",
        "+ 需要清除指标的状态时（通常在epoch末尾），调用`metric.reset_states()`\n",
        "\n",
        "让我们使用这些知识在每个epoch结束时，使用`SparseCategoricalAccuracy`计算验证数据："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty4U8ROpdujg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 创建模型\n",
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# 实例化用于训练模型的优化器\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# 实例化损失函数\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 准备指标\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# 准备训练数据\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# 准备验证数据集\n",
        "# 预留10,000个样本用于验证\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubE79pUjdvep",
        "colab_type": "text"
      },
      "source": [
        "这是我们的训练和评估循环："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtHBF9maiLnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "bb7da1b6-a568-4c12-f439-db1c8a163127"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 遍历数据集的批量\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # 更新训练指标\n",
        "        train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # 每200个批量记录一次\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
        "\n",
        "    # 在每个epoch结束后显示指标\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # 在每个epoch结束后重置指标\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # 在每个epoch结束后运行验证循环\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # 更新指标值\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 105.9828\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 1.3818\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 1.3547\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 1.0363\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.9450\n",
            "Seen so far: 51264 samples\n",
            "Training acc over epoch: 0.6877\n",
            "Validation acc: 0.8265\n",
            "Time taken: 8.47s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.7976\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.5406\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.6293\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.6605\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.5388\n",
            "Seen so far: 51264 samples\n",
            "Training acc over epoch: 0.8345\n",
            "Validation acc: 0.8751\n",
            "Time taken: 8.41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fEJZHViNU8",
        "colab_type": "text"
      },
      "source": [
        "### **使用tf.function加快训练步骤**\n",
        "\n",
        "TensorFlow 2.0中的默认运行时是Eager Execution（动态图）模式 。因此，我们上面的训练循环会以动态图模式执行。\n",
        "\n",
        "这对于调试非常有用，但是图形编译具有一定的性能优势，将你的计算描述为静态图可使框架应用全局性能优化。框架在不知道接下来会发生什么的情况下，是不可能一个接一个地执行操作。\n",
        "\n",
        "你可以将以张量为输入的任何函数编译为静态图，只需在其上添加一个`@tf.function`装饰器，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAgP7aRDiTEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJHieKKMiPmc",
        "colab_type": "text"
      },
      "source": [
        "让我们对评估步骤进行相同的操作："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzv6Xd7xiV0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " @tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFw6d6qIiXqF",
        "colab_type": "text"
      },
      "source": [
        "现在，让我们通过编译后的训练步骤重新运行训练循环："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xi-FDV5ibbE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "cb3702c2-7175-434c-f847-f1b219244d41"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 遍历数据集的批量\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # 每200个批量记录一次\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
        "\n",
        "    # 在每个epoch结束后显示指标\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # 在每个epoch结束后重置指标\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # 在每个epoch结束后运行验证循环\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 0.4298\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.5934\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.3899\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.5006\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.5795\n",
            "Seen so far: 51264 samples\n",
            "Training acc over epoch: 0.8699\n",
            "Validation acc: 0.8963\n",
            "Time taken: 1.97s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.2032\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 200: 0.2656\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 400: 0.6380\n",
            "Seen so far: 25664 samples\n",
            "Training loss (for one batch) at step 600: 0.9145\n",
            "Seen so far: 38464 samples\n",
            "Training loss (for one batch) at step 800: 0.6284\n",
            "Seen so far: 51264 samples\n",
            "Training acc over epoch: 0.8886\n",
            "Validation acc: 0.9111\n",
            "Time taken: 1.52s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gL-fhaJicvl",
        "colab_type": "text"
      },
      "source": [
        "快很多，不是吗？\n",
        "\n",
        "### **使用低级别方法处理模型跟踪的损失**\n",
        "\n",
        "层和模型递归地跟踪所有由调用`self.add_loss(value)`的层的前向传递过程中，创建的损失。标量损失值的结果列表可通过前向传递结束时的属性`model.losses`获得。\n",
        "\n",
        "如果要使用这些损失，则应将它们求和并将其添加到训练步骤的主要损失中。\n",
        "\n",
        "考虑下面的这个层，这会导致活动正则化损失："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMj3H7UIiirs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
        "        return inputs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbgv0dX1ij4O",
        "colab_type": "text"
      },
      "source": [
        "让我们构建非常简单的模型来使用它："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OLJxSp4imlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "# 将活动正则化作为一个层插入\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o98TQL7uinnp",
        "colab_type": "text"
      },
      "source": [
        "我们现在的训练步骤应为："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7B1npeCin7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "        # 添加在前向传递过程中创建的任何额外损失。\n",
        "        loss_value += sum(model.losses)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUpMP59NipWl",
        "colab_type": "text"
      },
      "source": [
        "### **总结**\n",
        "\n",
        "现在你学习到了使用内置训练循环以及从头开始编写自己的训练循环的所有知识。\n",
        "\n",
        "为了更好的理解，下面学习一个简单的端到端示例，将你在本指南中学到的所有内容联系在一起：一个通过MNIST数据集训练的DCGAN。\n",
        "\n",
        "### **端到端示例：从头开始编写GAN训练循环**\n",
        "你可能熟悉生成式对抗网络（GAN），通过学习图像训练数据集的潜在分布（图像的“潜在空间”），GAN可以生成看起来几乎真实的新图像。\n",
        "\n",
        "GAN由两部分组成：将潜在空间中的点映射到图像空间中的点的“生成器”模型，可以区分实际图像（来自训练数据集）以及假图片（生成器网络的输出）的“判别器”模型。\n",
        "\n",
        "GAN训练循环内容如下：\n",
        "\n",
        "1）训练判别器。 \n",
        "+ 在潜在空间中采样一批随机点\n",
        "+ 通过“生成器”模型将这些点转换为伪图像\n",
        "+ 获取一批真实图像，并将其与生成的图像合并\n",
        "+ 训练“判别器”模型对生成的图像与真实图像进行分类。\n",
        "\n",
        "2）训练生成器。\n",
        "+ 在潜在空间中采样随机点\n",
        "+ 通过“生成器”网络将点转换为假图像。\n",
        "+ 获取一批真实图像，并将其与生成的图像合并。\n",
        "+ 训练“生成器”模型以“欺骗”判别器，并将假图像分类为真实图像。\n",
        "\n",
        "有关GAN的工作原理的详细介绍，请参阅[《Python深度学习》](https://www.manning.com/books/deep-learning-with-python)。\n",
        "\n",
        "接下来让我们实现这个训练循环，首先，创建旨在区分假数据和实数据的判别器："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_NkOd5oixu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1694934b-6db2-44a2-c006-20b2979ff84c"
      },
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 74,625\n",
            "Trainable params: 74,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2pjCIOTizW1",
        "colab_type": "text"
      },
      "source": [
        "然后，让我们创建一个生成器网络，该网络将潜矢量转换为形状为`(28, 28, 1)`（代表MNIST数字）的输出："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcB22r6zi0ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # 我们想生成128个系数以reshape为7x7x128的图\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pt3LdJ3i10C",
        "colab_type": "text"
      },
      "source": [
        "下面的训练循环是个关键，如你所见，它非常简单，训练步长方法仅需17行。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNeCJBdOi3d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # 为判别器和生成器各实例化一个优化器\n",
        "d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)\n",
        "g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "# 实例化损失函数\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    # 在潜在空间中采样随机点\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # 将它们解码为假图\n",
        "    generated_images = generator(random_latent_vectors)\n",
        "    # 将它们与真图混合\n",
        "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "    # 整合区分真假图像的标签\n",
        "    labels = tf.concat(\n",
        "        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n",
        "    )\n",
        "    # 为标签添加噪音 - 非常重要的技巧\n",
        "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
        "\n",
        "    # 训练判别器\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(combined_images)\n",
        "        d_loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "    # 在潜在空间中采样随机点\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # 整合所有标有“真实图像”的标签\n",
        "    misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "    # 训练生成器（请注意，我们*不*更新判别器器的权重）！\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(generator(random_latent_vectors))\n",
        "        g_loss = loss_fn(misleading_labels, predictions)\n",
        "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
        "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "    return d_loss, g_loss, generated_images"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doe6UeFzi4z8",
        "colab_type": "text"
      },
      "source": [
        "让我们训练GAN，通过在图像批量上反复调用`train_step`。\n",
        "\n",
        "由于我们的判别器和生成器是卷积网络，因此你将要在GPU上运行此代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tl7nQ87i6JD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7c3b03d5-bd09-49d5-b9e7-33fe91a99da7"
      },
      "source": [
        "import os\n",
        "\n",
        "# 准备数据集，我们同时在MNIST数据集上训练模型.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 1  # 实际上，你至少需要20个epoch才能生成漂亮的数字。\n",
        "save_dir = \"./\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart epoch\", epoch)\n",
        "\n",
        "    for step, real_images in enumerate(dataset):\n",
        "        # 在一个批量的真实图片上训练判别器和生成器\n",
        "        d_loss, g_loss, generated_images = train_step(real_images)\n",
        "\n",
        "        # 记录\n",
        "        if step % 200 == 0:\n",
        "            # 打印指标\n",
        "            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
        "            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n",
        "\n",
        "            # 保存一个生成图片\n",
        "            img = tf.keras.preprocessing.image.array_to_img(\n",
        "                generated_images[0] * 255.0, scale=False\n",
        "            )\n",
        "            img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))\n",
        "\n",
        "        # 为了限制执行时间，我们在10个步骤后停止\n",
        "        if step > 10:\n",
        "            break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start epoch 0\n",
            "discriminator loss at step 0: 0.70\n",
            "adversarial loss at step 0: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOZpbqei7wg",
        "colab_type": "text"
      },
      "source": [
        "在Colab GPU上进行约30秒钟的训练后，你将获得漂亮的假的MNIST数字。"
      ]
    }
  ]
}