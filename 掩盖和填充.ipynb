{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "掩盖和填充.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeTZzr4Tm03hPh4epZ0S4c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5cr8hwBm_JR",
        "colab_type": "text"
      },
      "source": [
        "# **Keras的掩盖和填充**\n",
        "\n",
        "### **引入**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3F-LeY5mPbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnY2xPySnJOE",
        "colab_type": "text"
      },
      "source": [
        "### **介绍**\n",
        "\n",
        "**掩盖**是一种通知序列处理层在输入中，某些时间步丢失的方法，因此在处理数据时应将其跳过。\n",
        "\n",
        "**填充**是掩盖的一种特殊形式，位于掩盖步骤的开始或者序列的开始。填充是用于将序列数据编码为连续的批处理：为了使批处理中的所有序列都能达到标准长度，所以有必要填充或截断某些序列。\n",
        "\n",
        "接下来，让我们详细讨论。\n",
        "\n",
        "### **填充序列数据**\n",
        "处理序列数据时，每个样本具有不同的长度是很常见的。思考以下示例（标记为单词的文本）："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1JUCYKhnSyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[\n",
        "  [\"Hello\", \"world\", \"!\"],\n",
        "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
        "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6V8VhsGnUQo",
        "colab_type": "text"
      },
      "source": [
        "在进行单词检索之后，数据可能被矢量化为整数，例如："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Be_paInUqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[\n",
        "  [71, 1331, 4231]\n",
        "  [73, 8, 3215, 55, 927],\n",
        "  [83, 91, 1, 645, 1253, 927],\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z2nDwjpnXF0",
        "colab_type": "text"
      },
      "source": [
        "数据是一个嵌套列表，其中各个样本的长度分别为3、5和6，由于深度学习模型的输入数据必须是单个张量（例如，在这种情况下形状应为`(batch_size, 6, vocab_size)`），因此，较短的样本需要用一些占位符值填充（当然，也可以在填充之前截断长样本）。\n",
        "\n",
        "Keras提供了一个实用工具方法，可将Python列表截断并填充到相同的长度：[`tf.keras.preprocessing.sequence.pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtEwX9RXnZKn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a28df5e7-6b29-49b5-8c5f-1233aa467ae3"
      },
      "source": [
        "raw_inputs = [\n",
        "    [711, 632, 71],\n",
        "    [73, 8, 3215, 55, 927],\n",
        "    [83, 91, 1, 645, 1253, 927],\n",
        "]\n",
        "\n",
        "# 默认情况下，会使用0进行填充;你可以通过\"value\"属性进行配置\n",
        "# 你可以使用\"pre\"填充(在序列前填充)或者使用\"post\"填充(在序列尾填充)\n",
        "# 当使用RNN层时，我们推荐使用\"post\"填充(为了能够使用这些层的CuDNN实现)\n",
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    raw_inputs, padding=\"post\"\n",
        ")\n",
        "print(padded_inputs)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 711  632   71    0    0    0]\n",
            " [  73    8 3215   55  927    0]\n",
            " [  83   91    1  645 1253  927]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5h0aSRnaae",
        "colab_type": "text"
      },
      "source": [
        "### **掩盖**\n",
        "\n",
        "现在所有样本都具有统一的长度，必须告知模型该数据的某些部分实际上是填充的，应该忽略。这种机制正是**掩盖**。\n",
        "\n",
        "在Keras模型中，有三种方法可以引入输入掩盖：\n",
        "\n",
        "+ 添加一个[`keras.layers.Masking`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking)图层。\n",
        "+ 使用配置了`mask_zero=True`的[`keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)层。\n",
        "+ 在支持`mask`参数的层中，手动传递该参数（例如RNN层）。\n",
        "\n",
        "### **掩盖生成层：Embedding和Masking**\n",
        "\n",
        "这些层将创建一个掩盖张量（一个shape为`(batch, sequence_length)`的2D张量），并将其附加到`Masking`或`Embedding`层返回的张量输出上。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFnP2qfnnjdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "769caad4-3446-43ca-9f60-73aed44406d0"
      },
      "source": [
        "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "masked_output = embedding(padded_inputs)\n",
        "\n",
        "print(masked_output._keras_mask)\n",
        "\n",
        "masking_layer = layers.Masking()\n",
        "# 通过将2D输入扩展为3D（嵌入尺寸为10）来模拟嵌入查找。\n",
        "unmasked_embedding = tf.cast(\n",
        "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
        ")\n",
        "\n",
        "masked_embedding = masking_layer(unmasked_embedding)\n",
        "print(masked_embedding._keras_mask)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ True  True  True False False False]\n",
            " [ True  True  True  True  True False]\n",
            " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n",
            "tf.Tensor(\n",
            "[[ True  True  True False False False]\n",
            " [ True  True  True  True  True False]\n",
            " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDyDtPqNnkyZ",
        "colab_type": "text"
      },
      "source": [
        "从打印结果中可以看到，掩盖是shape为`(batch_size, sequence_length)`的2D布尔张量 ，其中每个False表示在处理过程中应忽略相应的时间步长。\n",
        "\n",
        "### **函数式API和Sequential API中的掩盖传播**\n",
        "\n",
        "使用函数式API或Sequential API时，由`Embedding`或`Masking`层生成的掩盖将通过网络传播到能够使用它们的层（例如，RNN层）。Keras将自动获取与输入相对应的掩盖，并将其传递给知道如何使用该掩盖的层。\n",
        "\n",
        "例如，在下面的Sequential模型中，LSTM层将自动接收掩盖，这意味着它将忽略填充值："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5NWKayGnpQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32),]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhbsi3WUnqcZ",
        "colab_type": "text"
      },
      "source": [
        "以下函数式API模型也是如此："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhVRtyX8nr6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "outputs = layers.LSTM(32)(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU3ZXOWCnuLe",
        "colab_type": "text"
      },
      "source": [
        "### **将掩盖张量直接传递到层**\n",
        "\n",
        "可以处理掩盖的层（例如`LSTM`层）在其`__call__`方法中具有一个`mask`参数。\n",
        "\n",
        "同时，产生掩盖的层（例如Embedding）会暴露`compute_mask(input, previous_mask)`方法，这样你可以调用该方法。\n",
        "\n",
        "因此，你可以将掩盖生成层的`compute_mask(`)方法的输出，传递给掩盖处理层的`__call__`方法，如下所示："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4wsHz2inz_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "eb504324-3002-4ef9-f901-2bb1dc841c8f"
      },
      "source": [
        "class MyLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyLayer, self).__init__(**kwargs)\n",
        "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
        "        self.lstm = layers.LSTM(32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        # 请注意，你也可以手动准备“mask”张量\n",
        "        # 它只需要是一个具有正确形状的布尔张量，即（batch_size，timesteps）\n",
        "        mask = self.embedding.compute_mask(inputs)\n",
        "        output = self.lstm(x, mask=mask)  # 层将会忽略掩盖值\n",
        "        return output\n",
        "\n",
        "\n",
        "layer = MyLayer()\n",
        "x = np.random.random((32, 10)) * 100\n",
        "x = x.astype(\"int32\")\n",
        "layer(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
              "array([[-2.9557333e-03, -4.9592513e-03, -5.5697327e-03, ...,\n",
              "        -6.4263460e-03,  1.7222196e-03, -5.9980587e-03],\n",
              "       [ 1.2630115e-03,  6.7719403e-03, -2.2096483e-06, ...,\n",
              "        -1.5865450e-06, -2.0998481e-03,  1.5725335e-03],\n",
              "       [-5.8411937e-03,  1.5372749e-03,  8.2000103e-03, ...,\n",
              "        -1.2324885e-03, -3.1526345e-03,  4.1902601e-04],\n",
              "       ...,\n",
              "       [-1.0838141e-03,  5.6212828e-03,  2.4681108e-03, ...,\n",
              "         9.0770936e-03, -4.8025837e-03,  8.2275616e-03],\n",
              "       [-5.9154308e-03,  2.6111908e-03, -5.9640841e-03, ...,\n",
              "        -1.1013175e-02, -2.0490203e-04, -3.4524666e-03],\n",
              "       [ 4.0665409e-03, -1.0666442e-03, -3.6945881e-03, ...,\n",
              "        -5.4598167e-03,  7.1055471e-04, -6.4708097e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ2NXn9In1-B",
        "colab_type": "text"
      },
      "source": [
        "### **在你自定义层中支持掩盖**\n",
        "\n",
        "有时，你可能需要编写生成掩盖的层（例如`Embedding`），或需要修改当前掩盖的层。\n",
        "\n",
        "例如，任何产生张量的时间维度与其输入不同的层，例如以时间维度串联的`Concatenate`层，都需要修改当前掩盖，以便下游层能够理解掩盖的时间步长。\n",
        "\n",
        "为此，你的层应实现该`layer.compute_mask()`方法，该方法将根据输入和当前掩盖生成一个新的掩盖。\n",
        "\n",
        "下面是一个`TemporalSplit`的示例，该层需要修改当前掩盖。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwdBD4srn8Gm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "1594548a-537d-48bf-b65b-22f558b7b720"
      },
      "source": [
        "class TemporalSplit(keras.layers.Layer):\n",
        "    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # 预期输入为3D，掩盖为2D，沿时间轴（轴1）将输入张量分成2个子张量\n",
        "        return tf.split(inputs, 2, axis=1)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # 如果存在的话，也将掩盖分成2个\n",
        "        if mask is None:\n",
        "            return None\n",
        "        return tf.split(mask, 2, axis=1)\n",
        "\n",
        "\n",
        "first_half, second_half = TemporalSplit()(masked_embedding)\n",
        "print(first_half._keras_mask)\n",
        "print(second_half._keras_mask)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ True  True  True]\n",
            " [ True  True  True]\n",
            " [ True  True  True]], shape=(3, 3), dtype=bool)\n",
            "tf.Tensor(\n",
            "[[False False False]\n",
            " [ True  True False]\n",
            " [ True  True  True]], shape=(3, 3), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOHdDqrYn-Fl",
        "colab_type": "text"
      },
      "source": [
        "下面是`CustomEmbedding`的一个示例，该层能够根据输入值生成掩盖："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4SNdQ2Wn_G9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "d4ac1aaf-d257-425d-d848-4632f10e5d3a"
      },
      "source": [
        "class CustomEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
        "        super(CustomEmbedding, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.mask_zero = mask_zero\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            shape=(self.input_dim, self.output_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if not self.mask_zero:\n",
        "            return None\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
        "x = np.random.random((3, 10)) * 9\n",
        "x = x.astype(\"int32\")\n",
        "\n",
        "y = layer(x)\n",
        "mask = layer.compute_mask(x)\n",
        "\n",
        "print(mask)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ True  True  True  True  True False  True  True  True  True]\n",
            " [ True  True  True  True  True  True  True  True False  True]\n",
            " [False  True  True False  True  True  True  True  True  True]], shape=(3, 10), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4UZ_pMYoCg1",
        "colab_type": "text"
      },
      "source": [
        "### **选择性使用兼容层上的掩盖传播**\n",
        "\n",
        "大多数层都不会修改时间维度，因此不需要修改当前掩盖。但是，他们可能仍然希望能够将当前的掩盖保持不变地**传播**到下一层。**这是一种选择性行为**，默认情况下，自定义层将破坏当前的掩盖（因为框架无法确定传播该掩盖是否安全）。\n",
        "\n",
        "如果你有一个不修改时间维度的自定义层，并且希望它能够传播当前输入掩盖，则应在层构造函数中设置`self.supports_masking = True`。在这种情况下，的默认行为`compute_mask()`是仅将当前掩盖传递下去。\n",
        "\n",
        "下面的示例展示将掩盖传播列入白名单的层："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSlG2T5NoHI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyActivation(keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyActivation, self).__init__(**kwargs)\n",
        "        # 表示该层可安全进行掩盖传播\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.nn.relu(inputs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3mnJYooIRo",
        "colab_type": "text"
      },
      "source": [
        "现在，你可以在生成掩盖的层（如`Embedding`）和使用掩盖的层（如`LSTM`）之间使用此自定义层，它将传递掩盖，使其到达使用掩盖的层。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5O5-tSEoJtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a486185b-f238-4ef9-acef-a5ae22897ccc"
      },
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
        "x = MyActivation()(x)  # 会直接传递掩盖\n",
        "print(\"Mask found:\", x._keras_mask)\n",
        "outputs = layers.LSTM(32)(x)  # 将会接收到掩盖\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mask found: Tensor(\"embedding_2/NotEqual:0\", shape=(None, None), dtype=bool)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WboDA-yyoLhE",
        "colab_type": "text"
      },
      "source": [
        "### **编写需要掩盖信息的层**\n",
        "\n",
        "有些层是掩盖的使用者：它们在`call`方法中接受一个`mask`参数，并用它来确定是否跳过某些时间步长。\n",
        "\n",
        "要编写这样的层，你只需在`call`方法定义中，添加一个`mask=None`参数即可。只要有，与输入关联的掩盖将被传递到你的层。\n",
        "\n",
        "下面是一个简单的示例：一个层，该层在输入序列的时间维度（轴1）上计算softmax，同时丢弃掩盖的时间步长。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leXxDwu6oP0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TemporalSoftmax(keras.layers.Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
        "        inputs_exp = tf.exp(inputs) * broadcast_float_mask\n",
        "        inputs_sum = tf.reduce_sum(inputs * broadcast_float_mask, axis=1, keepdims=True)\n",
        "        return inputs_exp / inputs_sum\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
        "x = layers.Dense(1)(x)\n",
        "outputs = TemporalSoftmax()(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNggxTLnoQ8n",
        "colab_type": "text"
      },
      "source": [
        "### **总结**\n",
        "\n",
        "以上就是你需要了解有关Keras中的填充和掩盖的全部信息，我们来回顾一下：\n",
        "\n",
        "+ “掩盖”是各层知道如何跳过/忽略序列输入中的某些时间步长的方法。\n",
        "+ 有些层是掩盖生成器：`Embedding`可以根据输入值（当`mask_zero=True`）生成掩盖，`Masking`层也同样可以。\n",
        "+ 有些层是掩盖的使用者：它们在其`__call__`方法中暴露一个`mask`参数，如RNN层。\n",
        "+ 在函数式API和Sequential API中，掩盖信息会自动传播。\n",
        "+ 当以独立方式使用层时，可以将`mask`参数手动传递给层。\n",
        "+ 你可以轻松地编写修改当前掩盖的层，生成新掩盖或使用与输入关联的掩盖的层。"
      ]
    }
  ]
}